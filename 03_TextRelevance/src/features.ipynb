{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPath = '../data'\n",
    "trainTfIdfPath = '../data/trainDataTfIdf'\n",
    "tfIdfVectorizerPath = '../data/models/tfIdfVectorizer.pkl'\n",
    "tfIdfNumberPagesPath = '../data/tfIdfNumberPages'\n",
    "tfIdfPagesPath = '../data/tfidfPages'\n",
    "tfIdfFeaturesPath = '../data/tfidfFeatutes'\n",
    "docUrlsPath = '../data/urls.docs.txt'\n",
    "textdataPath = '../data/textdata'\n",
    "templateJson = '{:d}.json'\n",
    "\n",
    "queriesDocsPath = '../data/queries.docs.txt'\n",
    "queriesPath = '../data/queries.numerate.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docDict = {}\n",
    "with open(docUrlsPath) as inputFile:\n",
    "    for line in inputFile:\n",
    "        number, url, path = line.strip().split('\\t')\n",
    "        docDict[int(number)] = (url, path)\n",
    "\n",
    "pat = re.compile(r'\\d+')\n",
    "procNumbs = sorted(list(map(lambda x: int(pat.search(x).group(0)), os.listdir(textdataPath))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение словаря: номер запроса: номера документов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "queryDocs = {}\n",
    "for numb in numbQueries:\n",
    "    queryDocs[int(numb)] = list(map(int, list(groupSample.get_group(numb)[\"DocumentId\"])))    \n",
    "json.dump(queryDocs, open(queriesDocsPath, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание обучающего множества для TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {\"numbs\":[], \"strings\":[]}\n",
    "sizeMemory = 0\n",
    "for numb in procNumbs:\n",
    "    filename = templateJson.format(numb)\n",
    "    path = '{:s}/{:s}'.format(textdataPath, filename)\n",
    "    pageDict = json.load(open(path))\n",
    "    text = pageDict['text'].get('p', [])\n",
    "    text = '\\n'.join(text)\n",
    "    \n",
    "    data[\"numbs\"].append(numb)\n",
    "    data[\"strings\"].append(text)\n",
    "    sizeMemory += len(text)\n",
    "    if sizeMemory > 10 ** 9:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.dump(data, open(trainTfIdfPath, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', '...гда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f896ce3bac8>>,\n",
       "        use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "rusStopwords = stopwords.words('russian')\n",
    "tokenizer = TweetTokenizer().tokenize\n",
    "tfIdfVect = TfidfVectorizer(tokenizer=tokenizer, stop_words=rusStopwords, max_features=50000000)\n",
    "tfIdfVect.fit(data[\"strings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Сохранение модели и результатов **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/tfIdfVectorizer.pkl']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tfIdfVect, tfIdfVectorizerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = tfIdfVect.transform(data[\"strings\"])\n",
    "json.dump(data[\"numbs\"], open(tfIdfNumberRagesPath, 'w'))\n",
    "json.dump(tfIdfVect.get_feature_names(), open(tfIdfFeaturesPath, 'w'))\n",
    "joblib.dump(result, tfIdfRagesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.dump(tfIdfVect.get_feature_names(), open(tfIdfFeaturesPath, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Вычисление tfidf по запросам **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pat = re.compile(r'\\d+')\n",
    "procNumbs = set(map(lambda x: int(pat.search(x).group(0)), os.listdir(textdataPath)))\n",
    "\n",
    "queriesDict = {}\n",
    "with open(queriesPath) as inputFile:\n",
    "    for line in inputFile:\n",
    "        number, query = line.strip().split('\\t')\n",
    "        queriesDict[number] = query\n",
    "        \n",
    "queriesDocsDict = json.load(open(queriesDocsPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfIdfBank:\n",
    "    tfIdfNumberPagesPath = '../data/tfIdfNumberPages'\n",
    "    tfIdfFeaturesPath = '../data/tfidfFeatutes'\n",
    "    \n",
    "    def __init__(self, tfIdfMatrix):\n",
    "        numberPages = json.load(open(TfIdfBank.tfIdfNumberPagesPath))\n",
    "        features = json.load(open(TfIdfBank.tfIdfFeaturesPath))\n",
    "        \n",
    "        self._indexPages = np.full(27000, -1, dtype=np.int)\n",
    "        self._indexPages[numberPages] = range(len(numberPages))\n",
    "        self._tfIdfMatrix = tfIdfMatrix#joblib.load(TfIdfFeatures.tfIdfPagesPath)        \n",
    "        self._indexFeatures = dict(map(lambda x: (x[1], x[0]), enumerate(features)))\n",
    "        \n",
    "    def tfidf(self, word, numberPage):\n",
    "        indexPage = self._indexPages[numberPage]\n",
    "        if indexPage == -1:\n",
    "            return None\n",
    "        indexFeature = self._indexFeatures.get(word.strip().lower(), -1)\n",
    "        if indexFeature == -1:\n",
    "            return 0\n",
    "        return self._tfIdfMatrix[indexPage, indexFeature]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfIdfForPagePath = '../data/features/tfidfFullText'\n",
    "tfidfFeatures = TfIdfFeatures()\n",
    "tfidfPages = {}\n",
    "for numb, query in queriesDict.items():\n",
    "    numbsDocs = queriesDocsDict[numb]\n",
    "    words = query.strip().split()\n",
    "    tfidfs = []\n",
    "    for numbDoc in numbsDocs:\n",
    "        if tfidfFeatures._indexPages[numbDoc] == -1:\n",
    "            tfidfs.append(None)\n",
    "        else:\n",
    "            tfidfs.append(sum(list(map(lambda x: tfidfFeatures.tfidf(x, numbDoc), words))))\n",
    "    tfidfPages[numb] = tfidfs\n",
    "json.dump(tfidfPages, open(tfIdfForPagePath, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение tfidf матриц от всех тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfIdfNumberPagesPath = '../data/tfIdfNumberPages'\n",
    "tfIdfFullPagesPath = '../data/tfidfFullPages'\n",
    "tfIdfVectorizerPath = '../data/models/tfIdfVectorizer.pkl'\n",
    "\n",
    "tfidfVectorizer = joblib.load(tfIdfVectorizerPath)\n",
    "tfIdfNumbers = json.load(open(tfIdfNumberPagesPath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, vstack\n",
    "tfidfMatrix = None\n",
    "data = []\n",
    "maxCount = 1000\n",
    "curCount = 0\n",
    "for numb in tfIdfNumbers:\n",
    "    filename = templateJson.format(numb)\n",
    "    path = '{:s}/{:s}'.format(textdataPath, filename)\n",
    "    pageDict = json.load(open(path))\n",
    "    text = []\n",
    "    for value in pageDict['text'].values():\n",
    "        text.extend(value)\n",
    "    text = '\\n'.join(text)    \n",
    "    data.append(text)\n",
    "    curCount += 1\n",
    "    if curCount > maxCount:\n",
    "        print(numb,)\n",
    "        if tfidfMatrix is None:\n",
    "            tfidfMatrix = tfidfVectorizer.transform(data)\n",
    "        else:\n",
    "            tfidfMatrix = vstack([tfidfMatrix, tfidfVectorizer.transform(data)])\n",
    "        data = []\n",
    "        curCount = 0\n",
    "\n",
    "if curCount > 0:\n",
    "    tfidfMatrix = vstack([tfidfMatrix, tfidfVectorizer.transform(data)])\n",
    "joblib.dump(tfidfMatrix, tfIdfFullPagesPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Вычисление tfidf по title, keywords, description **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textdataPath = '../data/textdata'\n",
    "titles = []\n",
    "keywords = []\n",
    "descriptions = []\n",
    "for numb in procNumbs:\n",
    "    filename = templateJson.format(numb)\n",
    "    path = '{:s}/{:s}'.format(textdataPath, filename)\n",
    "    pageDict = json.load(open(path))\n",
    "    titles.append(pageDict[\"title\"])\n",
    "    keywords.append(pageDict[\"keywords\"])\n",
    "    descriptions.append(pageDict[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(titles, open('../data/subdata/titles', 'w'))\n",
    "json.dump(keywords, open('../data/subdata/keywords', 'w'))\n",
    "json.dump(descriptions, open('../data/subdata/description', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = json.load(open('../data/subdata/titles'))\n",
    "keywords = json.load(open('../data/subdata/keywords'))\n",
    "descriptions = json.load(open('../data/subdata/description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfIdfVectorizerPath = '../data/models/tfIdfVectorizer.pkl'\n",
    "tfIdfVect = joblib.load(tfIdfVectorizerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfDicts = [{}, {}, {}]\n",
    "for i, data in enumerate([titles, keywords, descriptions]):\n",
    "    bank = TfIdfBank(tfIdfVect.transform(data))\n",
    "    for numb, query in queriesDict.items():\n",
    "        numbsDocs = queriesDocsDict[numb]\n",
    "        words = query.strip().split()\n",
    "        tfidfs = []\n",
    "        for numbDoc in numbsDocs:\n",
    "            if bank._indexPages[numbDoc] == -1:\n",
    "                tfidfs.append(None)\n",
    "            else:\n",
    "                tfidfs.append(sum(list(map(lambda x: bank.tfidf(x, numbDoc), words))))\n",
    "        tfidfDicts[i][numb] = tfidfs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titlePath = '../data/features/tfidfTitle'\n",
    "keywordsPath = '../data/features/tfidfKeywords'\n",
    "descPath = '../data/features/tfidfDesc'\n",
    "\n",
    "json.dump(tfidfDicts[0], open(titlePath, 'w'))\n",
    "json.dump(tfidfDicts[1], open(keywordsPath, 'w'))\n",
    "json.dump(tfidfDicts[2], open(descPath, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Стеминг данных **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "normalizedataPath = '../data/normalizedata'\n",
    "\n",
    "class TextNormalizer:\n",
    "    def __init__(self, stemmer, tokenizer):\n",
    "        self._stemmer = stemmer\n",
    "        self._tokenizer = tokenizer\n",
    "    def _stemTokens(self, tokens):\n",
    "        stemmed = []\n",
    "        for item in tokens:\n",
    "            stemmed.append(self._stemmer.stem(item))\n",
    "        return stemmed\n",
    "    def _tokenize(self, text):\n",
    "        tokens = self._tokenizer.tokenize(text)\n",
    "        stems = self._stemTokens(tokens)\n",
    "        return stems\n",
    "    def normalize(self, text):\n",
    "        return ' '.join(self._tokenize(text))\n",
    "\n",
    "textNormalizer = TextNormalizer(SnowballStemmer('russian'), TweetTokenizer())\n",
    "\n",
    "for numb in procNumbs:\n",
    "    if numb < 3830:\n",
    "        continue\n",
    "    filename = templateJson.format(numb)\n",
    "    path = '{:s}/{:s}'.format(textdataPath, filename)\n",
    "    pageDict = json.load(open(path))\n",
    "    \n",
    "    pageDict[\"description\"] = textNormalizer.normalize(pageDict[\"description\"])\n",
    "    pageDict[\"keywords\"] = textNormalizer.normalize(pageDict[\"keywords\"])\n",
    "    for key, texts in pageDict[\"text\"].items():\n",
    "        pageDict[\"text\"][key] = [textNormalizer.normalize(text) for text in texts]\n",
    "    for key, texts in pageDict[\"attr\"].items():\n",
    "        pageDict[\"attr\"][key] = [textNormalizer.normalize(text) for text in texts]\n",
    "    \n",
    "    outpath = '{:s}/{:s}'.format(normalizedataPath, filename)\n",
    "    json.dump(pageDict, open(outpath, 'w'))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
