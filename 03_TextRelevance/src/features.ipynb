{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPath = '../data'\n",
    "trainTfIdfPath = '../data/trainDataTfIdf'\n",
    "\n",
    "tfIdfNumberPagesPath = '../data/tfIdfNumberPages'\n",
    "tfIdfPagesPath = '../data/tfidfPages'\n",
    "tfIdfFeaturesPath = '../data/tfidfFeatutes'\n",
    "docUrlsPath = '../data/urls.docs.txt'\n",
    "textdataPath = '../data/textdata'\n",
    "templateJson = '{:d}.json'\n",
    "\n",
    "queriesDocsPath = '../data/queries.docs.txt'\n",
    "queriesPath = '../data/queries.numerate.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docsUrlsPath = '../data/urls.docs.txt'\n",
    "textdataPath = '../data/textdata'\n",
    "templateJson = '{:d}.json'\n",
    "queriesDocsPath = '../data/queries.docs.txt'\n",
    "queriesPath = '../data/queries.numerate.txt'\n",
    "\n",
    "pat = re.compile(r'\\d+')\n",
    "procNumbs = sorted(list(map(lambda x: int(pat.search(x).group(0)), os.listdir(textdataPath))))\n",
    "\n",
    "queriesDict = {}\n",
    "with open(queriesPath) as inputFile:\n",
    "    for line in inputFile:\n",
    "        number, query = line.strip().split('\\t')\n",
    "        queriesDict[number] = query\n",
    "        \n",
    "queriesDocsDict = json.load(open(queriesDocsPath))\n",
    "\n",
    "docsUrlsDict = {}\n",
    "with open(docsUrlsPath) as inputFile:\n",
    "    for line in inputFile:\n",
    "        number, url, path = line.strip().split('\\t')\n",
    "        docsUrlsDict[number] = (url, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание обучающего множества для TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {\"numbs\":[], \"strings\":[]}\n",
    "sizeMemory = 0\n",
    "for numb in procNumbs:\n",
    "    filename = templateJson.format(numb)\n",
    "    path = '{:s}/{:s}'.format(textdataPath, filename)\n",
    "    pageDict = json.load(open(path))\n",
    "    text = pageDict['text'].get('p', [])\n",
    "    text = '\\n'.join(text)\n",
    "    \n",
    "    data[\"numbs\"].append(numb)\n",
    "    data[\"strings\"].append(text)\n",
    "    sizeMemory += len(text)\n",
    "    if sizeMemory > 10 ** 9:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.dump(data, open(trainTfIdfPath, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', '...гда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f896ce3bac8>>,\n",
       "        use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "rusStopwords = stopwords.words('russian')\n",
    "tokenizer = TweetTokenizer().tokenize\n",
    "tfIdfVect = TfidfVectorizer(tokenizer=tokenizer, stop_words=rusStopwords, max_features=50000000)\n",
    "tfIdfVect.fit(data[\"strings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Сохранение модели и результатов **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/tfIdfVectorizer.pkl']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tfIdfVect, tfIdfVectorizerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = tfIdfVect.transform(data[\"strings\"])\n",
    "json.dump(data[\"numbs\"], open(tfIdfNumberRagesPath, 'w'))\n",
    "json.dump(tfIdfVect.get_feature_names(), open(tfIdfFeaturesPath, 'w'))\n",
    "joblib.dump(result, tfIdfRagesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.dump(tfIdfVect.get_feature_names(), open(tfIdfFeaturesPath, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Вычисление tfidf по запросам **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pat = re.compile(r'\\d+')\n",
    "procNumbs = set(map(lambda x: int(pat.search(x).group(0)), os.listdir(textdataPath)))\n",
    "\n",
    "queriesDict = {}\n",
    "with open(queriesPath) as inputFile:\n",
    "    for line in inputFile:\n",
    "        number, query = line.strip().split('\\t')\n",
    "        queriesDict[number] = query\n",
    "        \n",
    "queriesDocsDict = json.load(open(queriesDocsPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfIdfBank:\n",
    "    tfIdfNumberPagesPath = '../data/tfIdfNumberPages'\n",
    "    tfIdfFeaturesPath = '../data/tfidfFeatutes'\n",
    "    \n",
    "    def __init__(self, tfIdfMatrix):\n",
    "        numberPages = json.load(open(TfIdfBank.tfIdfNumberPagesPath))\n",
    "        features = json.load(open(TfIdfBank.tfIdfFeaturesPath))\n",
    "        \n",
    "        self._indexPages = np.full(27000, -1, dtype=np.int)\n",
    "        self._indexPages[numberPages] = range(len(numberPages))\n",
    "        self._tfIdfMatrix = tfIdfMatrix#joblib.load(TfIdfFeatures.tfIdfPagesPath)        \n",
    "        self._indexFeatures = dict(map(lambda x: (x[1], x[0]), enumerate(features)))\n",
    "        \n",
    "    def tfidf(self, word, numberPage):\n",
    "        indexPage = self._indexPages[numberPage]\n",
    "        if indexPage == -1:\n",
    "            return None\n",
    "        indexFeature = self._indexFeatures.get(word.strip().lower(), -1)\n",
    "        if indexFeature == -1:\n",
    "            return 0\n",
    "        return self._tfIdfMatrix[indexPage, indexFeature]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfIdfForPagePath = '../data/features/tfidfFullText'\n",
    "tfidfFeatures = TfIdfFeatures()\n",
    "tfidfPages = {}\n",
    "for numb, query in queriesDict.items():\n",
    "    numbsDocs = queriesDocsDict[numb]\n",
    "    words = query.strip().split()\n",
    "    tfidfs = []\n",
    "    for numbDoc in numbsDocs:\n",
    "        if tfidfFeatures._indexPages[numbDoc] == -1:\n",
    "            tfidfs.append(None)\n",
    "        else:\n",
    "            tfidfs.append(sum(list(map(lambda x: tfidfFeatures.tfidf(x, numbDoc), words))))\n",
    "    tfidfPages[numb] = tfidfs\n",
    "json.dump(tfidfPages, open(tfIdfForPagePath, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Сохранение tfidf матриц от всех тегов **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfIdfNumberPagesPath = '../data/tfIdfNumberPages'\n",
    "tfIdfFullPagesPath = '../data/tfidfFullPages'\n",
    "tfIdfVectorizerPath = '../data/models/tfIdfVectorizer.pkl'\n",
    "\n",
    "tfidfVectorizer = joblib.load(tfIdfVectorizerPath)\n",
    "tfIdfNumbers = json.load(open(tfIdfNumberPagesPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, vstack\n",
    "tfidfMatrix = None\n",
    "data = []\n",
    "maxCount = 1000\n",
    "curCount = 0\n",
    "for numb in tfIdfNumbers:\n",
    "    filename = templateJson.format(numb)\n",
    "    path = '{:s}/{:s}'.format(textdataPath, filename)\n",
    "    pageDict = json.load(open(path))\n",
    "    text = []\n",
    "    for value in pageDict['text'].values():\n",
    "        text.extend(value)\n",
    "    text = '\\n'.join(text)    \n",
    "    data.append(text)\n",
    "    curCount += 1\n",
    "    if curCount > maxCount:\n",
    "        print(numb,)\n",
    "        if tfidfMatrix is None:\n",
    "            tfidfMatrix = tfidfVectorizer.transform(data)\n",
    "        else:\n",
    "            tfidfMatrix = vstack([tfidfMatrix, tfidfVectorizer.transform(data)])\n",
    "        data = []\n",
    "        curCount = 0\n",
    "\n",
    "if curCount > 0:\n",
    "    tfidfMatrix = vstack([tfidfMatrix, tfidfVectorizer.transform(data)])\n",
    "joblib.dump(tfidfMatrix, tfIdfFullPagesPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Вычисление tfidf по title, keywords, description **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textdataPath = '../data/textdata'\n",
    "titles = []\n",
    "keywords = []\n",
    "descriptions = []\n",
    "for numb in procNumbs:\n",
    "    filename = templateJson.format(numb)\n",
    "    path = '{:s}/{:s}'.format(textdataPath, filename)\n",
    "    pageDict = json.load(open(path))\n",
    "    titles.append(pageDict[\"title\"])\n",
    "    keywords.append(pageDict[\"keywords\"])\n",
    "    descriptions.append(pageDict[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(titles, open('../data/subdata/titles', 'w'))\n",
    "json.dump(keywords, open('../data/subdata/keywords', 'w'))\n",
    "json.dump(descriptions, open('../data/subdata/description', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = json.load(open('../data/subdata/titles'))\n",
    "keywords = json.load(open('../data/subdata/keywords'))\n",
    "descriptions = json.load(open('../data/subdata/description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfIdfVectorizerPath = '../data/models/tfIdfVectorizer.pkl'\n",
    "tfIdfVect = joblib.load(tfIdfVectorizerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfDicts = [{}, {}, {}]\n",
    "for i, data in enumerate([titles, keywords, descriptions]):\n",
    "    bank = TfIdfBank(tfIdfVect.transform(data))\n",
    "    for numb, query in queriesDict.items():\n",
    "        numbsDocs = queriesDocsDict[numb]\n",
    "        words = query.strip().split()\n",
    "        tfidfs = []\n",
    "        for numbDoc in numbsDocs:\n",
    "            if bank._indexPages[numbDoc] == -1:\n",
    "                tfidfs.append(None)\n",
    "            else:\n",
    "                tfidfs.append(sum(list(map(lambda x: bank.tfidf(x, numbDoc), words))))\n",
    "        tfidfDicts[i][numb] = tfidfs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titlePath = '../data/features/tfidfTitle'\n",
    "keywordsPath = '../data/features/tfidfKeywords'\n",
    "descPath = '../data/features/tfidfDesc'\n",
    "\n",
    "json.dump(tfidfDicts[0], open(titlePath, 'w'))\n",
    "json.dump(tfidfDicts[1], open(keywordsPath, 'w'))\n",
    "json.dump(tfidfDicts[2], open(descPath, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Стеминг данных **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "normalizedataPath = '../data/normalizedata'\n",
    "\n",
    "class TextNormalizer:\n",
    "    def __init__(self, stemmer, tokenizer):\n",
    "        self._stemmer = stemmer\n",
    "        self._tokenizer = tokenizer\n",
    "    def _stemTokens(self, tokens):\n",
    "        stemmed = []\n",
    "        for item in tokens:\n",
    "            stemmed.append(self._stemmer.stem(item))\n",
    "        return stemmed\n",
    "    def _tokenize(self, text):\n",
    "        tokens = self._tokenizer.tokenize(text)\n",
    "        stems = self._stemTokens(tokens)\n",
    "        return stems\n",
    "    def normalize(self, text):\n",
    "        return ' '.join(self._tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textNormalizer = TextNormalizer(SnowballStemmer('russian'), TweetTokenizer())\n",
    "\n",
    "for numb in procNumbs:\n",
    "    if numb < 7090:\n",
    "        continue\n",
    "    filename = templateJson.format(numb)\n",
    "    path = '{:s}/{:s}'.format(textdataPath, filename)\n",
    "    pageDict = json.load(open(path))\n",
    "    \n",
    "    pageDict[\"title\"] = textNormalizer.normalize(pageDict[\"title\"])\n",
    "    pageDict[\"description\"] = textNormalizer.normalize(pageDict[\"description\"])\n",
    "    pageDict[\"keywords\"] = textNormalizer.normalize(pageDict[\"keywords\"])\n",
    "    for key, texts in pageDict[\"text\"].items():\n",
    "        pageDict[\"text\"][key] = [textNormalizer.normalize(text) for text in texts]\n",
    "    for key, texts in pageDict[\"attr\"].items():\n",
    "        pageDict[\"attr\"][key] = [textNormalizer.normalize(text) for text in texts]\n",
    "    \n",
    "    outpath = '{:s}/{:s}'.format(normalizedataPath, filename)\n",
    "    json.dump(pageDict, open(outpath, 'w'))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Обучение Tfidf **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание обучающего множества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tags = [\"p\", \"div\", \"br\", \"span\", \"img\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"b\", \"strong\", \"i\"]\n",
    "dataPath = '../data/textdata'\n",
    "data = []\n",
    "sizeMemory = 0\n",
    "\n",
    "for numb in list(np.random.choice(np.array(procNumbs), size=len(procNumbs), replace=False)):\n",
    "    filename = templateJson.format(numb)\n",
    "    path = '{:s}/{:s}'.format(dataPath, filename)\n",
    "    pageDict = json.load(open(path))\n",
    "    text = []\n",
    "    for tag in filter(lambda x: x in tags, pageDict['text'].keys()):\n",
    "        text.extend(pageDict['text'][tag])\n",
    "    text.extend([pageDict['title'], pageDict['description'], pageDict['keywords']])\n",
    "    text = '\\n'.join(text)     \n",
    "    data.append(text)\n",
    "    sizeMemory += len(text)\n",
    "    if sizeMemory > 5 * (10 ** 8):\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPath = '../data/trainNorNorm'\n",
    "json.dump(data, open(dataPath, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '../data/trainNorNorm'\n",
    "data = json.load(open(dataPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer  = SnowballStemmer('russian')\n",
    "rusStopwords = [stemmer.stem(word) for word in stopwords.words('russian')]\n",
    "\n",
    "tokenizer = TweetTokenizer().tokenize\n",
    "tfIdfVect = TfidfVectorizer(tokenizer=tokenizer, stop_words=rusStopwords, max_features=1000000)\n",
    "tfIdfVect.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/randomNotNorm2.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "rusStopwords = stopwords.words('russian')\n",
    "tokenizer = TweetTokenizer().tokenize\n",
    "tfIdfVect = TfidfVectorizer(tokenizer=tokenizer, stop_words=rusStopwords, max_features=1000000)\n",
    "tfIdfVect.fit(data)\n",
    "tfIdfVectorizerPath = '../data/models/randomNotNorm2.pkl' \n",
    "joblib.dump(tfIdfVect, tfIdfVectorizerPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечение фич"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class QueryHandler:\n",
    "    def __init__(self, tags):\n",
    "        self._textNormalizer = TextNormalizer(SnowballStemmer('russian'), TweetTokenizer())\n",
    "        self._tags = tags\n",
    "        self._docs = []\n",
    "        self._texts = []\n",
    "        self._queries = []\n",
    "    def clean(self):\n",
    "        self._docs = []\n",
    "        self._texts = []\n",
    "        self._queries = []\n",
    "    def addDoc(self, query, doc):\n",
    "        curTags = list(filter(lambda x: x in self._tags, doc['text'].keys()))\n",
    "        curNames = [\"text\", \"title\", \"keywords\", 'description']\n",
    "        text = []\n",
    "        for tag in curTags:\n",
    "            text.append('\\n'.join(doc['text'][tag]))\n",
    "        self._texts.append('\\n'.join(text))\n",
    "        self._texts.extend([doc['title'], \n",
    "                            doc['keywords'], \n",
    "                            doc['description']]) \n",
    "        self._queries.extend([queriesDict[query]] * 4)\n",
    "        self._docs.append({\"names\":curNames, \"number\":doc[\"number\"], \"query\":query})\n",
    "    def queriesAndTexts(self):\n",
    "         return (self._queries, self._texts)\n",
    "    def accordForTags(self, scores):\n",
    "        scoreDict = defaultdict(dict)\n",
    "        curI = 0\n",
    "        for doc in self._docs:\n",
    "            curDict = {}\n",
    "            for tag in doc[\"names\"]:\n",
    "                curDict[tag] = scores[curI]\n",
    "                curI += 1\n",
    "            scoreDict[doc[\"query\"]][doc[\"number\"]] = curDict\n",
    "        return scoreDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "class TfidfHandler:\n",
    "    def __init__(self, vectorizer):        \n",
    "        self._vectorizer = vectorizer\n",
    "        features = tfIdfVect.get_feature_names()\n",
    "        self._indexFeatures = dict(map(lambda x: (x[1], x[0]), enumerate(features)))\n",
    "    def scores(self, queries, texts):\n",
    "        score = []\n",
    "        matrix = self._vectorizer.transform(texts)\n",
    "        for i, query in enumerate(queries):        \n",
    "            wordsQuery = query.strip().split()\n",
    "            curScore = 0\n",
    "            for word in wordsQuery:\n",
    "                j = self._indexFeatures.get(word, None)\n",
    "                if j is None:\n",
    "                    continue \n",
    "                curScore += matrix[i, j]\n",
    "            score.append(curScore)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfIdfVectorizerPath = '../data/models/randomNotNorm.pkl' \n",
    "tfIdfVect = joblib.load(tfIdfVectorizerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textNormalizer = TextNormalizer(SnowballStemmer('russian'), TweetTokenizer())\n",
    "for numbQuery, query in queriesDict.items():\n",
    "    queriesDict[numbQuery] = textNormalizer.normalize(query)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "tags = [\"p\", \"div\", \"br\", \"span\", \"img\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"b\", \"strong\", \"i\"]\n",
    "dataPath = '../data/textdata'\n",
    "\n",
    "tfidfsForQuery = defaultdict(dict)\n",
    "queryHandler = QueryHandler(tags)\n",
    "tfidfHandler = TfidfHandler(tfIdfVect)\n",
    "maxCount = 1000\n",
    "curCount = 0\n",
    "\n",
    "for numbQuery, query in queriesDict.items():\n",
    "    numbsDocs = queriesDocsDict[numbQuery]\n",
    "    for numbDoc in numbsDocs:\n",
    "        if not(numbDoc in procNumbs):\n",
    "            tfidfsForQuery[numbQuery][numbDoc] = None\n",
    "            continue\n",
    "        filename = templateJson.format(numbDoc)\n",
    "        path = '{:s}/{:s}'.format(dataPath, filename)\n",
    "        doc = json.load(open(path))\n",
    "        queryHandler.addDoc(numbQuery, doc)\n",
    "        curCount +=1\n",
    "        if curCount >= maxCount:\n",
    "            print('Ok')\n",
    "            xqueries, xtexts = queryHandler.queriesAndTexts()\n",
    "            xscores = tfidfHandler.scores(xqueries, xtexts)\n",
    "            xscores = queryHandler.accordForTags(xscores)\n",
    "            for nq, xdocs in xscores.items():\n",
    "                for nd, s in xdocs.items():\n",
    "                    tfidfsForQuery[nq][nd] = s            \n",
    "            curCount = 0\n",
    "            queryHandler.clean()\n",
    "            \n",
    "if curCount > 0:\n",
    "    xqueries, xtexts = queryHandler.queriesAndTexts()\n",
    "    xscores = tfidfHandler.scores(xqueries, xtexts)\n",
    "    xscores = queryHandler.accordForTags(xscores)\n",
    "    for nq, xdocs in xscores.items():\n",
    "        for nd, s in xdocs.items():\n",
    "            tfidfsForQuery[nq][nd] = s\n",
    "thidfsPath = '../data/features/tfidfsNotNorm'\n",
    "json.dump(tfidfsForQuery, open(tfidfsPath, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
