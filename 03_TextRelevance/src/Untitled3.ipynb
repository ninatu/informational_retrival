{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docsUrlsPath = '../data/urls.docs.txt'\n",
    "textdataPath = '../data/textdata'\n",
    "templateJson = '{:d}.json'\n",
    "queriesDocsPath = '../data/queries.docs.txt'\n",
    "queriesPath = '../data/queries.numerate.txt'\n",
    "\n",
    "pat = re.compile(r'\\d+')\n",
    "procNumbs = set(map(lambda x: int(pat.search(x).group(0)), os.listdir(textdataPath)))\n",
    "\n",
    "queriesDict = {}\n",
    "with open(queriesPath) as inputFile:\n",
    "    for line in inputFile:\n",
    "        number, query = line.strip().split('\\t')\n",
    "        queriesDict[number] = query\n",
    "        \n",
    "queriesDocsDict = json.load(open(queriesDocsPath))\n",
    "\n",
    "docsUrlsDict = {}\n",
    "with open(docsUrlsPath) as inputFile:\n",
    "    for line in inputFile:\n",
    "        number, url, path = line.strip().split('\\t')\n",
    "        docsUrlsDict[number] = (url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfIdfBank:\n",
    "    tfIdfNumberPagesPath = '../data/tfIdfNumberPages'\n",
    "    tfIdfFeaturesPath = '../data/tfidfFeatutes'\n",
    "    \n",
    "    def __init__(self, tfIdfMatrix):\n",
    "        numberPages = json.load(open(TfIdfBank.tfIdfNumberPagesPath))\n",
    "        features = json.load(open(TfIdfBank.tfIdfFeaturesPath))\n",
    "        \n",
    "        self._indexPages = np.full(27000, -1, dtype=np.int)\n",
    "        self._indexPages[numberPages] = range(len(numberPages))\n",
    "        self._tfIdfMatrix = tfIdfMatrix#joblib.load(TfIdfFeatures.tfIdfPagesPath)        \n",
    "        self._indexFeatures = dict(map(lambda x: (x[1], x[0]), enumerate(features)))\n",
    "        \n",
    "    def tfidf(self, word, numberPage):\n",
    "        indexPage = self._indexPages[numberPage]\n",
    "        if indexPage == -1:\n",
    "            return None\n",
    "        indexFeature = self._indexFeatures.get(word.strip().lower(), -1)\n",
    "        if indexFeature == -1:\n",
    "            return 0\n",
    "        return self._tfIdfMatrix[indexPage, indexFeature]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bold  = []\n",
    "inclined = []\n",
    "header = []\n",
    "for numb in procNumbs:\n",
    "    filename = templateJson.format(numb)\n",
    "    path = '{:s}/{:s}'.format(textdataPath, filename)\n",
    "    pageDict = json.load(open(path))\n",
    "    tests = pageDict[\"text\"].get('b', [''[])\n",
    "    bold.append(pageDict[\"text\"].get('b', ''))\n",
    "    keywords.append(pageDict[\"keywords\"])\n",
    "    descriptions.append(pageDict[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.dump(titles, open('../data/subdata/titles', 'w'))\n",
    "json.dump(keywords, open('../data/subdata/keywords', 'w'))\n",
    "json.dump(descriptions, open('../data/subdata/description', 'w'))\n",
    "json.dump(procNumbs, open('../data/subdata/procNumbs', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = json.load(open('../data/subdata/titles'))\n",
    "keywords = json.load(open('../data/subdata/keywords'))\n",
    "descriptions = json.load(open('../data/subdata/description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfIdfVectorizerPath = '../data/models/tfIdfVectorizer.pkl'\n",
    "tfIdfVect = joblib.load(tfIdfVectorizerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfDicts = [{}, {}, {}]\n",
    "for i, data in enumerate([titles, keywords, descriptions]):\n",
    "    bank = TfIdfBank(tfIdfVect.transform(data))\n",
    "    for numb, query in queriesDict.items():\n",
    "        numbsDocs = queriesDocsDict[numb]\n",
    "        words = query.strip().split()\n",
    "        tfidfs = []\n",
    "        for numbDoc in numbsDocs:\n",
    "            if bank._indexPages[numbDoc] == -1:\n",
    "                tfidfs.append(None)\n",
    "            else:\n",
    "                tfidfs.append(sum(list(map(lambda x: bank.tfidf(x, numbDoc), words))))\n",
    "        tfidfDicts[i][numb] = tfidfs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titlePath = '../data/features/tfidfTitle'\n",
    "keywordsPath = '../data/features/tfidfKeywords'\n",
    "descPath = '../data/features/tfidfDesc'\n",
    "\n",
    "json.dump(tfidfDicts[0], open(titlePath, 'w'))\n",
    "json.dump(tfidfDicts[1], open(keywordsPath, 'w'))\n",
    "json.dump(tfidfDicts[2], open(descPath, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {\"numbs\":[], \"strings\":[]}\n",
    "sizeMemory = 0\n",
    "for numb in procNumbs:\n",
    "    filename = templateJson.format(numb)\n",
    "    path = '{:s}/{:s}'.format(textdataPath, filename)\n",
    "    pageDict = json.load(open(path))\n",
    "    text = []\n",
    "    for value in pageDict['text'].values():\n",
    "        text.extend(value)\n",
    "    text = '\\n'.join(text)     \n",
    "    data[\"numbs\"].append(numb)\n",
    "    data[\"strings\"].append(text)\n",
    "    sizeMemory += len(text)\n",
    "    if sizeMemory > 2 * 10 ** 9:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "rusStopwords = stopwords.words('russian')\n",
    "tokenizer = TweetTokenizer().tokenize\n",
    "tfIdfVect = TfidfVectorizer(tokenizer=tokenizer, stop_words=rusStopwords, max_features=50000000)\n",
    "tfIdfVect.fit(data[\"strings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfIdfVectorizerPath = '../data/models/tfIdfFullText.pkl'\n",
    "joblib.dump(tfIdfVect, tfIdfVectorizerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и и\n",
      "в в\n",
      "во во\n",
      "не не\n",
      "что что\n",
      "он он\n",
      "на на\n",
      "я я\n",
      "с с\n",
      "со со\n",
      "как как\n",
      "а а\n",
      "то то\n",
      "все все\n",
      "она он\n",
      "так так\n",
      "его ег\n",
      "но но\n",
      "да да\n",
      "ты ты\n",
      "к к\n",
      "у у\n",
      "же же\n",
      "вы вы\n",
      "за за\n",
      "бы бы\n",
      "по по\n",
      "только тольк\n",
      "ее е\n",
      "мне мне\n",
      "было был\n",
      "вот вот\n",
      "от от\n",
      "меня мен\n",
      "еще ещ\n",
      "нет нет\n",
      "о о\n",
      "из из\n",
      "ему ем\n",
      "теперь тепер\n",
      "когда когд\n",
      "даже даж\n",
      "ну ну\n",
      "вдруг вдруг\n",
      "ли ли\n",
      "если есл\n",
      "уже уж\n",
      "или ил\n",
      "ни ни\n",
      "быть быт\n",
      "был был\n",
      "него нег\n",
      "до до\n",
      "вас вас\n",
      "нибудь нибуд\n",
      "опять опя\n",
      "уж уж\n",
      "вам вам\n",
      "ведь вед\n",
      "там там\n",
      "потом пот\n",
      "себя себ\n",
      "ничего нич\n",
      "ей е\n",
      "может может\n",
      "они он\n",
      "тут тут\n",
      "где где\n",
      "есть ест\n",
      "надо над\n",
      "ней не\n",
      "для для\n",
      "мы мы\n",
      "тебя теб\n",
      "их их\n",
      "чем чем\n",
      "была был\n",
      "сам сам\n",
      "чтоб чтоб\n",
      "без без\n",
      "будто будт\n",
      "чего чег\n",
      "раз раз\n",
      "тоже тож\n",
      "себе себ\n",
      "под под\n",
      "будет будет\n",
      "ж ж\n",
      "тогда тогд\n",
      "кто кто\n",
      "этот этот\n",
      "того тог\n",
      "потому пот\n",
      "этого эт\n",
      "какой как\n",
      "совсем совс\n",
      "ним ним\n",
      "здесь зде\n",
      "этом эт\n",
      "один один\n",
      "почти почт\n",
      "мой мо\n",
      "тем тем\n",
      "чтобы чтоб\n",
      "нее не\n",
      "сейчас сейчас\n",
      "были был\n",
      "куда куд\n",
      "зачем зач\n",
      "всех всех\n",
      "никогда никогд\n",
      "можно можн\n",
      "при при\n",
      "наконец наконец\n",
      "два два\n",
      "об об\n",
      "другой друг\n",
      "хоть хот\n",
      "после посл\n",
      "над над\n",
      "больше больш\n",
      "тот тот\n",
      "через через\n",
      "эти эт\n",
      "нас нас\n",
      "про про\n",
      "всего всег\n",
      "них них\n",
      "какая как\n",
      "много мног\n",
      "разве разв\n",
      "три три\n",
      "эту эт\n",
      "моя мо\n",
      "впрочем впроч\n",
      "хорошо хорош\n",
      "свою сво\n",
      "этой эт\n",
      "перед перед\n",
      "иногда иногд\n",
      "лучше лучш\n",
      "чуть чут\n",
      "том том\n",
      "нельзя нельз\n",
      "такой так\n",
      "им им\n",
      "более бол\n",
      "всегда всегд\n",
      "конечно конечн\n",
      "всю всю\n",
      "между межд\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer  = SnowballStemmer('russian')\n",
    "for word in stopwords.words('russian'):\n",
    "    print(word, stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tfIdfVectorizerPath = '../data/models/randomNotNorm.pkl' \n",
    "joblib.load(tfIdfVectorizerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
