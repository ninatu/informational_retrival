{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from math import log2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainPath = \"../data/train.data.cvs\"\n",
    "rowData = DataFrame.from_csv(trainPath, index_col=False).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relNormalize(rel, max_rel=20):\n",
    "    norm_rel = rel - min(rel)\n",
    "    if max(norm_rel) != 0:\n",
    "        norm_rel = norm_rel * max_rel / max(norm_rel)\n",
    "    return norm_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QueryData:\n",
    "    def __init__(self, X, rel, T_NDCG=5):\n",
    "        self._X = np.copy(X)\n",
    "        self._n = self._X.shape[0]\n",
    "        self._rel = np.copy(rel)\n",
    "        self._T_NDCG = T_NDCG\n",
    "        self._predcomputeNDCG()\n",
    "        \n",
    "    def _predcomputeNDCG(self):        \n",
    "        right_order = np.argsort(self._rel)[::-1]\n",
    "        self._X = self._X[right_order]    \n",
    "        self._rel = self._rel[right_order]\n",
    "        self._2_rel = 2 ** self._rel - 1   \n",
    "        log2_order = np.log2(np.array(range(self._n)) + 2)\n",
    "        self._max_DCG = sum(self._2_rel[:self._T_NDCG] / log2_order[:self._T_NDCG])\n",
    "   \n",
    "    def getX(self):\n",
    "        return self._X\n",
    "    \n",
    "    def getY(self):\n",
    "        return self._rel\n",
    "        \n",
    "    def getSwapNDCGMatrix(self, rel):\n",
    "        order = np.array(np.argsort(rel)[::-1]) + 1\n",
    "        log2_order = np.log2(order + 1)\n",
    "        elem_DCG = self._2_rel / log2_order\n",
    "        matr_elem_DCG = np.tile(elem_DCG, (self._n, 1))\n",
    "        matr_swap_elem_DCG = self._2_rel.reshape((self._n, 1)) / log2_order.reshape((1, self._n)) \n",
    "        \n",
    "        DCG = sum(elem_DCG[order <= self._T_NDCG])        \n",
    "        lambda_mtr = np.full((self._n, self._n), DCG)\n",
    "        lambda_mtr = lambda_mtr - matr_elem_DCG - matr_elem_DCG.T + matr_swap_elem_DCG + matr_swap_elem_DCG.T\n",
    "        no_null_swap = ((order <= self._T_NDCG).reshape((self._n, 1)) + (order <= self._T_NDCG).reshape((1, self._n))) > 0 \n",
    "        lambda_mtr = lambda_mtr * no_null_swap\n",
    "        if self._max_DCG != 0:\n",
    "            return lambda_mtr / self._max_DCG\n",
    "        else:\n",
    "            return lambda_mtr\n",
    "    \n",
    "    def getNDCG(self, rel):\n",
    "        order = np.array(np.argsort(rel)[::-1]) + 1\n",
    "        log2_order = np.log2(order + 1)\n",
    "        elem_DCG = self._2_rel / log2_order\n",
    "        DCG = sum(elem_DCG[order <= self._T_NDCG])\n",
    "        if self._max_DCG != 0:\n",
    "            return DCG / self._max_DCG\n",
    "        else:\n",
    "            return DCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QueryData:\n",
    "    def __init__(self, X, rel):\n",
    "        #uniq_rel = np.unique(rel)\n",
    "        self._X = np.copy(X)\n",
    "        self._n = self._X.shape[0]\n",
    "        self._rel = np.copy(rel)\n",
    "        \n",
    "        self._right_order = np.argsort(self._rel)[::-1]\n",
    "        self._2_rel = 2 ** self._rel[self._right_order] - 1   \n",
    "        self._log2_order = np.log2(np.array(range(self._n)) + 2)\n",
    "        \n",
    "    def maxDCG(self, T_NDCG=5):        \n",
    "        return sum(self._2_rel[:T_NDCG] / self._log2_order[:T_NDCG])\n",
    "   \n",
    "    def getX(self):\n",
    "        return self._X\n",
    "    \n",
    "    def getY(self):\n",
    "        return self._rel\n",
    "        \n",
    "    def getSwapNDCGMatrix(self, rel, T_NDCG):\n",
    "        rel = rel[self._right_order]\n",
    "        order = np.array(np.argsort(rel)[::-1]) + 1\n",
    "        log2_order = np.log2(order + 1)\n",
    "        elem_DCG = self._2_rel / log2_order\n",
    "        matr_elem_DCG = np.tile(elem_DCG, (self._n, 1))\n",
    "        matr_swap_elem_DCG = self._2_rel.reshape((self._n, 1)) / log2_order.reshape((1, self._n)) \n",
    "        \n",
    "        DCG = sum(elem_DCG[order <= T_NDCG])        \n",
    "        lambda_mtr = np.full((self._n, self._n), DCG)\n",
    "        lambda_mtr = lambda_mtr - matr_elem_DCG - matr_elem_DCG.T + matr_swap_elem_DCG + matr_swap_elem_DCG.T\n",
    "        no_null_swap = ((order <= T_NDCG).reshape((self._n, 1)) + (order <= T_NDCG).reshape((1, self._n))) > 0 \n",
    "        lambda_mtr = lambda_mtr * no_null_swap\n",
    "        max_DCG = self.maxDCG(T_NDCG)\n",
    "        if max_DCG != 0:\n",
    "            return lambda_mtr / max_DCG \n",
    "        else:\n",
    "            return lambda_mtr\n",
    "    \n",
    "    def getNDCG(self, rel, T_NDCG):\n",
    "        rel = rel[self._right_order]\n",
    "        order = np.array(np.argsort(rel)[::-1]) + 1\n",
    "        log2_order = np.log2(order + 1)\n",
    "        elem_DCG = self._2_rel / log2_order\n",
    "        DCG = sum(elem_DCG[order <= T_NDCG])\n",
    "        max_DCG = self.maxDCG(T_NDCG)\n",
    "        if max_DCG  != 0:\n",
    "            return DCG / max_DCG\n",
    "        else:\n",
    "            return DCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries = rowData[:, -1]\n",
    "uniq_queries = np.unique(queries)\n",
    "queries_train_data = []\n",
    "for q in uniq_queries:\n",
    "    xy = rowData[queries == q][:, :-1]\n",
    "    queries_train_data.append(QueryData(xy[:, 1:], relNormalize(xy[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor as DT\n",
    "\n",
    "class LambdaRankTrees :\n",
    "    def __init__ (self, learning_rate= 1, n_estimators=100, sigma=1, start_depth=6) :\n",
    "        self._learning_rate = learning_rate\n",
    "        self._n_estimators = n_estimators\n",
    "        self._sigma = sigma\n",
    "        self._start_depth = start_depth\n",
    "        self._trees = None\n",
    "        \n",
    "    def _getTrainX(self, queries_data):\n",
    "        n = 0\n",
    "        m = queries_data[0].getX().shape[1]\n",
    "        for query_data in queries_data:\n",
    "            n += query_data._n\n",
    "            \n",
    "        X = np.empty((n, m), dtype=np.float64)\n",
    "        indexs = []\n",
    "        cur_index = 0\n",
    "        for query_data in queries_data:\n",
    "            cur_n = query_data._n\n",
    "            X[cur_index:cur_index + cur_n] = query_data.getX()\n",
    "            indexs.append(range(cur_index,cur_index + cur_n))\n",
    "            cur_index += cur_n            \n",
    "        return X, indexs\n",
    "    \n",
    "    def _getGradient(self, queries_data, h, indexs_data, T_NDCG):\n",
    "        g = np.empty(h.shape[0], dtype=np.float64)\n",
    "        for i, indexs in enumerate(indexs_data):\n",
    "            query_data = queries_data[i]\n",
    "            rel = h[indexs]\n",
    "            rel_n = rel.shape[0]\n",
    "            lambda_matr = -self._sigma / (1 + np.exp(rel.reshape((rel_n, 1)) - rel.reshape((1, rel_n)))) * query_data.getSwapNDCGMatrix(rel, T_NDCG)\n",
    "            tril = np.tril(lambda_matr, k=-1)\n",
    "            lambda_vector =  np.sum(tril, axis=0) - np.sum(tril.T, axis=0)\n",
    "            g[indexs] = lambda_vector\n",
    "        return g\n",
    "    \n",
    "    def _getNDCG(self, queries_data, h, indexs_data, T_NDCG):\n",
    "        ndcg = 0\n",
    "        for i, indexs in enumerate(indexs_data):\n",
    "            query_data = queries_data[i]\n",
    "            rel = h[indexs]\n",
    "            ndcg+= query_data.getNDCG(rel, T_NDCG)\n",
    "        return ndcg / len(indexs_data)\n",
    "    \n",
    "    def fit(self, queries_data, max_add_iteration=10, persent_valid=0.4) :\n",
    "        random.shuffle(queries_data)        \n",
    "        count_valid = int(persent_valid * len(queries_data))\n",
    "        data_valid = queries_data[:count_valid]\n",
    "        data_train = queries_data[count_valid:]\n",
    "        \n",
    "        X_train, index_train = self._getTrainX(data_train)\n",
    "        X_valid, index_valid = self._getTrainX(data_valid)\n",
    "        \n",
    "        self._trees = []\n",
    "        #d_tree = DT(max_depth=self._start_depth)\n",
    "        #d_tree.fit(X_train, np.zeros(X_train.shape[0]))        \n",
    "        #self._trees.append(d_tree)\n",
    "        h_train = np.zeros(X_train.shape[0]) #d_tree.predict(X_train) * self._learning_rate   \n",
    "        h_valid = np.zeros(X_valid.shape[0])# d_tree.predict(X_valid) * self._learning_rate \n",
    "        \n",
    "        for iteration in range(self._n_estimators - 1) :\n",
    "            g = self._getGradient(data_train, h_train, index_train, 200)\n",
    "            norm_g = np.linalg.norm(g)            \n",
    "            \"\"\"\n",
    "            if norm_g > 3 :\n",
    "                d_tree = dt.DecisionTree('mse', self.max_depth, collect_gains_features=self.collect_gains_features)\n",
    "            else :\n",
    "                d_tree = dt.DecisionTree('mse', 3 * self.max_depth, collect_gains_features=self.collect_gains_features)\n",
    "            \"\"\"\n",
    "            #data = np.hstack((x, g.reshape(N, 1)))\n",
    "            d_tree = DT(max_depth=self._start_depth)\n",
    "            d_tree.fit(X_train, -g)\n",
    "            self._trees.append(d_tree)\n",
    "            h_train += self._learning_rate * d_tree.predict(X_train)\n",
    "            h_valid += self._learning_rate * d_tree.predict(X_valid)\n",
    "            \n",
    "            print(iteration, self._getNDCG(data_train, h_train, index_train, 200), \n",
    "                  self._getNDCG(data_valid, h_valid, index_valid, 200),\n",
    "                  self._getNDCG(data_train, h_train, index_train, 5),\n",
    "                  self._getNDCG(data_valid, h_valid, index_valid, 5),\n",
    "                  min(g), max(g), norm_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.882471607383 0.879794641397 0.741851134576 0.740358946969 -270.981913805 401.031132521 29728.3836268\n",
      "1 0.882428976474 0.880380766102 0.74222474288 0.740586307633 -783.041411882 770.93422236 42820.7121491\n",
      "2 0.882653677363 0.880254591426 0.742135524183 0.740382867256 -632.092835535 571.400331479 43922.8359138\n",
      "3 0.882400103584 0.880259655014 0.741518487671 0.739525252042 -783.029525045 571.400331725 43926.5813899\n",
      "4 0.882363880771 0.880266743657 0.74150282205 0.73979063436 -783.029525045 571.400331725 43888.832861\n",
      "5 0.882359116691 0.88027220467 0.741382649227 0.740086147941 -783.029525045 571.400331725 43870.2334001\n",
      "6 0.882327324095 0.880175583719 0.741333997737 0.740026604929 -783.029525045 571.400331725 43870.6745319\n",
      "7 0.882451283207 0.88018186143 0.741590369662 0.739843051202 -783.029525045 571.400331725 43870.6385878\n",
      "8 0.882440015156 0.880188768761 0.741531078937 0.739897779362 -783.029525045 571.400331725 43878.6311246\n",
      "9 0.882425763131 0.880169630663 0.74149054313 0.739843124136 -783.029525045 571.400331725 43866.6442192\n",
      "10 0.882394089756 0.880140550122 0.741594106025 0.739873072495 -783.029525045 571.400331725 43869.934649\n",
      "11 0.88239241895 0.88014468716 0.741584813265 0.739888246325 -783.029525045 571.400331725 43846.9139547\n",
      "12 0.882390449776 0.880110037453 0.741583914999 0.739713406068 -783.029525045 571.400331725 43848.80221\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d04aa8d07ff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlambdaRank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambdaRankTrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlambdaRank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-fa64b3770da6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, queries_data, max_add_iteration, persent_valid)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_estimators\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mnorm_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \"\"\"\n",
      "\u001b[0;32m<ipython-input-6-fa64b3770da6>\u001b[0m in \u001b[0;36m_getGradient\u001b[0;34m(self, queries_data, h, indexs_data, T_NDCG)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mrel_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mlambda_matr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sigma\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mquery_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSwapNDCGMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_NDCG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mtril\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_matr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlambda_vector\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambdaRank = LambdaRankTrees()\n",
    "lambdaRank.fit(queries_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20.          20.          20.          20.          20.          20.          20.\n",
      "  20.          20.          20.          20.          20.          20.          20.\n",
      "  20.          20.           0.          20.          20.          20.          20.\n",
      "  20.           6.66666667  20.          20.          20.          20.          20.\n",
      "  20.          20.          13.33333333  20.          20.          20.          20.\n",
      "  20.          20.          20.          13.33333333  13.33333333  20.          20.\n",
      "  20.          20.          20.          13.33333333  13.33333333  20.          20.\n",
      "  20.          20.           0.          20.          20.          20.          20.\n",
      "  20.          20.          20.          20.          20.          20.          20.\n",
      "  20.          20.          20.          20.          20.          20.          20.\n",
      "  20.          20.          20.          20.          20.          20.          20.\n",
      "  20.          20.          20.           0.          20.          20.          20.\n",
      "  13.33333333  20.        ]\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "query = queries_train_data[0]\n",
    "print(query._rel)\n",
    "print(query._n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66083979472638399"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.getNDCG(np.zeros(86))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13.33333333,  13.33333333,   6.66666667,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        13.33333333,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,   0.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  13.33333333,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,   0.        ,  20.        ,\n",
       "        20.        ,   0.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  13.33333333,  20.        ,  20.        ,\n",
       "        13.33333333,  20.        ,  20.        ,  20.        ,\n",
       "        20.        ,  20.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query._rel[query._right_order - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel = np.zeros(86)\n",
    "rel = rel[query._right_order]\n",
    "order = np.array(np.argsort(rel)[::-1]) + 1\n",
    "log2_order = np.log2(order + 1)\n",
    "elem_DCG = query._2_rel / log2_order\n",
    "DCG = sum(elem_DCG[order <= query._T_NDCG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3091680.52058 2043105.52058\n"
     ]
    }
   ],
   "source": [
    "print(query._max_DCG, DCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18080475.7176 17075306.3328 0.944405810969\n"
     ]
    }
   ],
   "source": [
    "print(query._max_DCG, DCG, DCG/query._max_DCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "\n",
    "    def predict(self, x) :\n",
    "        y = np.zeros(x.shape[0])\n",
    "        for tree in self.trees:\n",
    "            y = y + self.learning_rate * tree.predict(x)\n",
    "\n",
    "        return np.apply_along_axis(lambda x: (1.0 / (1 + np.exp(-x)) > 0.5) * 1.0, 0, y)\n",
    "\n",
    "    def score(self, x, y) :\n",
    "        h = np.zeros(x.shape[0])\n",
    "        score_y = []\n",
    "        for tree in self.trees:\n",
    "            h = h + self.learning_rate * tree.predict(x)\n",
    "            y_pred = np.apply_along_axis(lambda x: (1.0 / (1 + np.exp(-x)) > 0.5) * 1.0, 0, h)\n",
    "            score_y.append(accuracy_score(y, y_pred))\n",
    "        return np.array(score_y)\n",
    "        \n",
    "    def f1_score(self, x, y) :\n",
    "        h = np.zeros(x.shape[0])\n",
    "        score_y = []\n",
    "        for tree in self.trees:\n",
    "            h = h + self.learning_rate * tree.predict(x)\n",
    "            y_pred = np.apply_along_axis(lambda x: (1.0 / (1 + np.exp(-x)) > 0.5) * 1.0, 0, h)\n",
    "            score_y.append(f1_score(y, y_pred))\n",
    "        return np.array(score_y)\n",
    "        \n",
    "    def f1_macro(self, x, y) :\n",
    "        h = np.zeros(x.shape[0])\n",
    "        score_y = []\n",
    "        score_y1 = []\n",
    "        score_y2 = []\n",
    "        for tree in self.trees:\n",
    "            h = h + self.learning_rate * tree.predict(x)\n",
    "            y_pred = np.apply_along_axis(lambda x: (1.0 / (1 + np.exp(-x)) > 0.5) * 1.0, 0, h)\n",
    "            labels = unique_labels(y, y_pred)\n",
    "            f1 = f1_score(y, y_pred, labels=labels, average=None)\n",
    "            f1_macro = 0\n",
    "            for i, label in enumerate(labels):\n",
    "                f1_macro += f1[i]    \n",
    "            score_y1.append(f1[0])\n",
    "            score_y2.append(f1[1])\n",
    "            score_y.append(f1_macro)\n",
    "        return np.array(score_y)/ 2.0, np.array(score_y1), np.array(score_y2)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.argsort(np.isnan(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0, 289029, 289028, 289027, 289026, 289025, 289024, 289023,\n",
       "       289022, 289021])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([61491, 61492, 61493, 61494, 61495, 61496, 61497, 61498, 61499,\n",
       "       61500, 61501, 61502, 61503, 61504, 61505, 61506, 61507, 61508,\n",
       "       61509, 61510, 61511, 61512, 61513, 61515, 61488, 61544, 61546,\n",
       "       61576, 61577, 61578, 61579, 61580, 61581, 61582, 61583, 61584,\n",
       "       61585, 61586, 61587, 61588, 61589, 61590, 61591, 61592, 61593,\n",
       "       61594, 61595, 61596, 61597, 61598])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[-200:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from operator import add, mul, sub\n",
    "from math import exp, ceil, log\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2grey\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LambdaRankEstimator:\n",
    "    def __init__(self) :\n",
    "        pass\n",
    "    def init(self, count_input, count_layers=1, act_funcs=None, der_act_func=None,\n",
    "             count_neurals_layer=None, learning_rate=0.1, shuffle = True) :\n",
    "        defalt_count_neural = 10\n",
    "        if count_neurals_layer is None:\n",
    "            count_neurals_layer = []\n",
    "            for l in range(count_layers):\n",
    "                count_neurals_layer.append(defalt_count_neural)\n",
    "        if act_funcs is not None :\n",
    "            self.act_funcs = act_funcs\n",
    "            self.der_act_func = der_act_func\n",
    "        else :   \n",
    "            self.act_funcs = []\n",
    "            self.der_act_func = []\n",
    "            for l in range(count_layers):\n",
    "                self.act_funcs.append(logistic_activation_1) # max_0)\n",
    "                self.der_act_func.append(der_logistic_activation_1) # der_max_0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.count_output = 1\n",
    "        self.shuffle = shuffle\n",
    "        count_neurals_layer.append(self.count_output)\n",
    "        self.act_funcs.append(equal_func)\n",
    "        self.der_act_func.append(equal_func)\n",
    "        #self.function_error = mean_square\n",
    "        #self.dE_dz_last = der_mean_square - надо заменить!!!!!!!!!!!!!!!\n",
    "        self.create_network(count_input, count_layers + 1, count_neurals_layer)\n",
    "        self.initialize()\n",
    "\n",
    "    def fit(self, data, add_step = 3, add_iteration=100, max_epoche=3000, sigma=1,\n",
    "            coeff_R1=0, coeff_R2=0) :\n",
    "        self.coeff_R1 = coeff_R1\n",
    "        self.begin_coeff_R1 = coeff_R1\n",
    "        self.coeff_R2 = coeff_R2\n",
    "        self.begin_coeff_R2 = coeff_R2\n",
    "        self.validation_error = []\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(data)\n",
    "        \n",
    "        count_valid = int(4 * len(data) / 10)\n",
    "        data_valid = data[:count_valid]\n",
    "        data_train = data[count_valid:]\n",
    "\n",
    "        best_error = 1e100\n",
    "        best_weight = None\n",
    "        number_epoche = None\n",
    "        cur_step = 0\n",
    "        current_epoche = 0\n",
    "        while True :\n",
    "            #epoche\n",
    "            random.shuffle(data_train)\n",
    "            epoche_error = 0\n",
    "            for iteration in range(len(data_train)):#range(self.batch_count) :\n",
    "                x = data_train[iteration][:, :-1]\n",
    "                y_true = data_train[iteration][:, -1]\n",
    "                y_pred = self.forward_propogation(x)\n",
    "                print(y_pred.shape, y_true.shape)\n",
    "                break\n",
    "            break\n",
    "\n",
    "            \"\"\"\n",
    "            self.init_add_weight()\n",
    "            error = 0\n",
    "            for j in range(size_batch) :\n",
    "                n = iteration * size_batch + j\n",
    "                answer = self.forward_propogation(data_train[n])\n",
    "                self.back_propogation(data_train[n], answer, y_train[n])\n",
    "                error = error + self.function_error(y_train[n], answer)\n",
    "            self.add_mean_gradient(size_batch)\n",
    "            epoche_error = epoche_error + error\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            epoche_error = epoche_error/ (self.batch_count * size_batch)\n",
    "            print \"Epoche error\", epoche_error, \n",
    "            \n",
    "            error = 0\n",
    "            answer = self._predict_(data_x_valid)\n",
    "            for i, y in enumerate(answer):\n",
    "                error = error + self.function_error(y_valid[i], y)\n",
    "            error = error / answer.shape[0]            \n",
    "            print \"Validation error\", error\n",
    "            \n",
    "            if error <= best_error:\n",
    "              #  print \"New best error\", error\n",
    "                best_error = error\n",
    "                best_weight = self.get_copy_weight(self.weight_network)\n",
    "                self.validation_error.append(error)\n",
    "                number_epoche = current_epoche\n",
    "                \n",
    "            \n",
    "            if  number_epoche + add_iteration <= current_epoche:# and cur_step < add_step:\n",
    "                cur_step = cur_step + 1                \n",
    "                self.weight_network = self.get_copy_weight(best_weight)\n",
    "                self.learning_rate = self.learning_rate / 2.0\n",
    "                number_epoche = current_epoche\n",
    "                print \"New learning rate\", self.learning_rate\n",
    "                if cur_step > add_step:\n",
    "                    self.count_epoche_for_learn = current_epoche - 1\n",
    "                    break\n",
    "            \n",
    "            self.coeff_R1 = self.begin_coeff_R1 / ((current_epoche) ** 3 + 1)\n",
    "            self.coeff_R2 = self.begin_coeff_R2 / ((current_epoche) ** 3 + 1)\n",
    "            for i, x in enumerate(self.KL_probability):\n",
    "                if x is not None:\n",
    "                    prob, coeff = self.KL_begin_probability[i]\n",
    "                    self.KL_probability[i] = (prob, coeff / ((current_epoche) ** 3 + 1))\n",
    "\n",
    "            current_epoche = current_epoche + 1    \n",
    "            if current_epoche > max_epoche:\n",
    "                self.weight_network = self.get_copy_weight(best_weight)\n",
    "                self.count_epoche_for_learn = current_epoche - 1\n",
    "                break\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "    def get_copy_weight(self, weights) :\n",
    "        new_weight = []\n",
    "        for w in weights:\n",
    "            new_weight.append(w.copy())\n",
    "        return new_weight\n",
    "        \n",
    "    def _predict_(self, data) :\n",
    "        y = np.empty((data.shape[0], self.count_output))\n",
    "        for i, x in enumerate(data):\n",
    "            y[i, :] = self.forward_propogation(x)\n",
    "        return y\n",
    "        \n",
    "    def predict(self, data):\n",
    "        y_answer = self._predict_(data)\n",
    "        if self.isClassification:\n",
    "            y_pred = np.empty(y_answer.shape[0])\n",
    "            for i, y in enumerate(y_answer):            \n",
    "                y_pred[i] = self.uniq_label[y.argmax()]\n",
    "            return y_pred        \n",
    "        return y_answer\n",
    "    \n",
    "    def init_add_weight(self) :\n",
    "        self.add_weight = []\n",
    "        for layer in self.weight_network:\n",
    "            self.add_weight.append(np.zeros(layer.shape))\n",
    "\n",
    "    def create_network(self, count_input, count_layers, count_neurals_layer) :\n",
    "        self.weight_network = []\n",
    "        self.KL_probability = []\n",
    "        self.KL_begin_probability = []\n",
    "        count_neurals_layer = count_neurals_layer[:]\n",
    "        count_neurals_layer.insert(0, count_input)    \n",
    "        for l in range(1, count_layers + 1) :\n",
    "            self.weight_network.append(np.zeros((count_neurals_layer[l], count_neurals_layer[l-1])))\n",
    "            self.KL_probability.append(None)\n",
    "            self.KL_begin_probability.append(None)\n",
    "\n",
    "    def initialize(self, mean=0, var=1.0/800) :\n",
    "        for l, layer in enumerate(self.weight_network) :\n",
    "            var = 1.0 / layer.shape[1] #/ 10.0 #* 25\n",
    "            weights = [[random.gauss(mean, var) for i in range(layer.shape[1])] for j in range(layer.shape[0])]\n",
    "            self.weight_network[l] = np.array(weights)\n",
    "\n",
    "    def set_KL_probality(self, layer, prob, coeff):\n",
    "        self.KL_probability[layer] = (prob, coeff)\n",
    "        self.KL_begin_probability[layer] = (prob, coeff)\n",
    "        \n",
    "    def forward_propogation(self, data) :\n",
    "        cur_x = data.T\n",
    "        self.x = []\n",
    "        self.dy_dz = []\n",
    "        self.x.append(cur_x)\n",
    "        for l, weight_matrix in enumerate(self.weight_network):\n",
    "            z = np.dot(weight_matrix, cur_x)\n",
    "            func = (self.act_funcs)[l]\n",
    "            der_func = self.der_act_func[l]\n",
    "            cur_x = func(z)\n",
    "            self.x.append(cur_x)\n",
    "            self.dy_dz.append(der_func(z))\n",
    "        return cur_x\n",
    "        \n",
    "    def back_propogation(self, data, answer, true_y) :\n",
    "        dE_dz = self.dE_dz_last(true_y, answer)        \n",
    "        l = len(self.add_weight)\n",
    "        prev_x = self.x[l - 1]\n",
    "        dE_dz = dE_dz.reshape((dE_dz.shape[0], 1))\n",
    "        prev_x = prev_x.reshape((1, prev_x.shape[0]))\n",
    "        self.add_weight[l - 1] = self.add_weight[l - 1]  + np.dot(dE_dz, prev_x\n",
    "                ) + self.coeff_R1 * sign(self.weight_network[l - 1]\n",
    "                ) + self.coeff_R2 * self.weight_network[l - 1]\n",
    "        for i in range(l - 2, -1, -1):\n",
    "            tmp1_matrix = np.dot(self.weight_network[i+1].T , dE_dz)\n",
    "            prev_x = self.x[i]\n",
    "            prev_x = prev_x.reshape((1, prev_x.shape[0]))\n",
    "            dy_dz = self.dy_dz[i]\n",
    "            dy_dz = dy_dz.reshape((dy_dz.shape[0], 1))\n",
    "            tmp2_matrix = np.dot(dy_dz, prev_x)\n",
    "            add_weight = tmp2_matrix * tmp1_matrix  \n",
    "            add_R1 = self.coeff_R1 * sign(self.weight_network[i]) \n",
    "            add_R2 = self.coeff_R2 * self.weight_network[i]         \n",
    "            if self.KL_probability[i] is not None:\n",
    "                prob, coeff = self.KL_probability[i]\n",
    "                add_dE_dz = - dy_dz * (log(prob) / self.weight_network[i].shape[0])\n",
    "                add_KL_reg = np.dot(add_dE_dz, prev_x) * coeff\n",
    "            else:\n",
    "                add_dE_dz = 0\n",
    "                add_KL_reg = 0\n",
    "            add_weight = add_weight + add_R1 + add_R2 + add_KL_reg       \n",
    "            \n",
    "            self.add_weight[i] = self.add_weight[i]  + add_weight\n",
    "            dE_dz = tmp1_matrix * dy_dz + add_dE_dz\n",
    "            \n",
    "    def add_mean_gradient(self, count_iteration):\n",
    "        for i, layer in enumerate(self.add_weight) : \n",
    "           # print \"layer\" , i\n",
    "           # print \"w\", self.weight_network[i][:3, :3]\n",
    "           # print \"add\", (- self.learning_rate * layer / count_iteration)[:3, :3]\n",
    "            self.weight_network[i] = self.weight_network[i] - self.learning_rate * layer / count_iteration\n",
    "      \n",
    "    def print_weight(self):\n",
    "        for l, layer in enumerate(self.weight_network) :   \n",
    "            print(\"LAYER:\", l, \"shape\", layer.shape)\n",
    "            for n, neuron in enumerate(layer):\n",
    "                print(\"neuron:\", n)\n",
    "                for weight in neuron:\n",
    "                    print(weight)\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimator = LambdaRankEstimator()\n",
    "estimator.init(data[0].shape[1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 109) (109,)\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sign(data):\n",
    "    return (data > 0) * 1.0  - (data < 0) * 1.0\n",
    "\n",
    "def mean_square(y, answer):\n",
    "    return sum((y - answer) ** 2) / 2.0\n",
    "def der_mean_square(y, answer):\n",
    "    return answer - y\n",
    "    \n",
    "def entropy(y, answer) :\n",
    "    return sum(-y * np.log(answer + 1e-20))\n",
    "def der_soft_max(y, answer):\n",
    "    return answer - y\n",
    "    \n",
    "def equal_func(z) :\n",
    "    return z  \n",
    "def one_func (z) :\n",
    "    return 1.0;\n",
    "    \n",
    "def logistic_activation_1(z) :\n",
    "    return logistic_activation_a(1.0, z)\n",
    "def der_logistic_activation_1(z) :\n",
    "    return der_logistic_activation_a(1.0, z)\n",
    "    \n",
    "def max_0(z) :\n",
    "    return (z > 0) * z\n",
    "def der_max_0(z):\n",
    "    return (z > 0) * 1.0\n",
    "\n",
    "                \n",
    "def logistic_activation_a(a, z) :\n",
    "    return 1.0 / (1 + np.exp(-a * z))\n",
    "def der_logistic_activation_a(a, z):\n",
    "    return a*logistic_activation_a(a, z)*(1.0 - logistic_activation_a(a, z))\n",
    "    \n",
    "def load_data() :\n",
    "    path = './big_alphabet_29x29/mutant-'\n",
    "    count_char = 25\n",
    "    count_example = 8\n",
    "    image = rgb2grey(imread('./big_alphabet_29x29/mutant-0-0-0.bmp'))\n",
    "    size = image.shape[0] * image.shape[1]\n",
    "    data_x = np.zeros((count_char * count_example, size))\n",
    "    y = np.zeros(count_char * count_example)    \n",
    "\n",
    "        \n",
    "    for char in range(count_char) :\n",
    "        for i in range(count_example):\n",
    "            path_img = path + str(char) + '-' + str(i) + '-0.bmp'\n",
    "            data_x[char * count_example + i, :] = rgb2grey(imread(path_img)).reshape(size)\n",
    "            y[char * count_example + i] = char\n",
    "    data_x =  data_x - 0.5\n",
    "  #  data_x = data_x / np.max(np.abs(data_x))\n",
    "    return data_x, y \n",
    "    \n",
    "def acurancy(y_pred, y_true):\n",
    "    return float(sum(y_pred == y_true)) / y_pred.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
